{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = Path.cwd().parents[2]\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(str(module_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p metrics/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from time import perf_counter\n",
    "\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.tokens import Doc, DocBin, Span\n",
    "from spacy.training import Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_PATH = Path.cwd() / \"metrics\"\n",
    "METRICS_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loader import TextLoader\n",
    "from src.model import DatasetType, Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTED_EXAMPLES_PATH = Path(\"data/predicted_examples.pkl\")\n",
    "if not PREDICTED_EXAMPLES_PATH.exists():\n",
    "    # For faster calculations, will fail after it passes calculations and saves the data to pkl\n",
    "    # but at this point the data is saved and can be loaded from the file, so we can use CPU\n",
    "    torch.cuda.empty_cache()\n",
    "    spacy.require_gpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(dataset_type=DatasetType.V1_WITH_PREDICTIONSTRING)\n",
    "nlp = spacy.load(\"models/spacy_resume/model-best/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_manual_doc(text: Text):\n",
    "    word_idx = []\n",
    "    for disc in text.discourses:\n",
    "        word_idx.extend((disc.predictionstring[0], disc.predictionstring[-1]))\n",
    "\n",
    "    ents = []\n",
    "\n",
    "    DS_token = \"B-DS\"\n",
    "    DE_token = \"B-DE\"\n",
    "    use_DS = True\n",
    "    for ind, word in enumerate(text.words):\n",
    "        if use_DS:\n",
    "            curr_token = DS_token\n",
    "        else:\n",
    "            curr_token = DE_token\n",
    "\n",
    "        if ind in word_idx:\n",
    "            ents.append(curr_token)\n",
    "            use_DS = not use_DS\n",
    "        else:\n",
    "            ents.append(\"O\")\n",
    "\n",
    "    return Doc(nlp.vocab, text.words, ents=ents)\n",
    "\n",
    "\n",
    "def display_doc(doc: Doc):\n",
    "    spacy.displacy.render(doc, style=\"ent\", jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bin = DocBin().from_disk(\"data/NER_test.spacy\")\n",
    "len(doc_bin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_tokenization(doc: Doc) -> Doc:\n",
    "    \"\"\"Fix tokenization of reference doc.\"\"\"\n",
    "    tokens = []\n",
    "    spaces = []\n",
    "    ents = []\n",
    "    for token in doc:\n",
    "        tokenized = list(nlp(token.text))\n",
    "\n",
    "        if len(tokenized) == 1:\n",
    "            tokens.append(token.text)\n",
    "            spaces.append(True if token.whitespace_ else False)\n",
    "            if token.ent_iob_ == \"O\":\n",
    "                ents.append(token.ent_iob_)\n",
    "                continue\n",
    "\n",
    "            ents.append(f\"{token.ent_iob_}-{token.ent_type_}\")\n",
    "        else:\n",
    "            is_outside = not token.ent_type_\n",
    "            for ind, tok in enumerate(tokenized):\n",
    "                tokens.append(tok.text)\n",
    "                spaces.append(True if tok.whitespace_ else False)\n",
    "\n",
    "                if is_outside:\n",
    "                    ents.append(\"O\")\n",
    "                else:\n",
    "                    if ind == 0:\n",
    "                        ents.append(f\"B-{token.ent_type_}\")\n",
    "                    else:\n",
    "                        ents.append(f\"I-{token.ent_type_}\")\n",
    "\n",
    "    return Doc(nlp.vocab, words=tokens, spaces=spaces, ents=ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dddd = fix_tokenization(predicted_examples[92].reference)\n",
    "\n",
    "# pre = predicted_examples[92].predicted\n",
    "\n",
    "# for ind, ref_token in enumerate(dddd):\n",
    "#     pre_token = pre[ind]\n",
    "#     print(f\"{ref_token.text:14} {pre_token.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PREDICTED_EXAMPLES_PATH.exists():\n",
    "    predicted_examples: list[Example] = []\n",
    "    references = list(doc_bin.get_docs(nlp.vocab))\n",
    "    for ind, reference in enumerate(references):\n",
    "        print(f\"\\r{ind + 1:3d}/{len(references)}\", end=\"\")\n",
    "        doc = nlp(reference.text)\n",
    "        \n",
    "        reference_fixed = fix_tokenization(reference)\n",
    "        assert len(reference_fixed.ents) == len(reference.ents), \"Number of entities do not match!\"\n",
    "        assert [ent.text for ent in reference_fixed.ents] == [ent.text for ent in reference.ents], \"Entites do not match!\"\n",
    "\n",
    "        predicted_examples.append(Example(doc, reference_fixed))\n",
    "\n",
    "    pickle.dump(predicted_examples, open(PREDICTED_EXAMPLES_PATH, \"wb\"))\n",
    "\n",
    "    raise Exception(\"Data saved, restart kernel to run on CPU\")\n",
    "\n",
    "else:\n",
    "    with open(PREDICTED_EXAMPLES_PATH, \"rb\") as f:\n",
    "        predicted_examples: list[Example] = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, token in enumerate(predicted_examples[9].reference):\n",
    "    if not token.ent_type_:\n",
    "        continue\n",
    "\n",
    "    print(f\"{ind:3}\", f\"{token.text:14}\", f\"{token.ent_iob_}-{token.ent_type_}\")\n",
    "\n",
    "print()\n",
    "\n",
    "test_doc = fix_tokenization(predicted_examples[9].reference)\n",
    "for ind, token in enumerate(test_doc):\n",
    "    if not token.ent_type_:\n",
    "        continue\n",
    "\n",
    "    print(f\"{ind:3}\", f\"{token.text:14}\", f\"{token.ent_iob_}-{token.ent_type_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fixed_doc(example: Example, idx: list[int]):\n",
    "    doc = example.predicted\n",
    "    ents = doc.ents\n",
    "\n",
    "    tokens_fixed = []\n",
    "    tokens_spaces = []\n",
    "    ents_fixed = []\n",
    "    curr_ind = idx.pop(0)\n",
    "    for token in doc:\n",
    "        if curr_ind is not None and token.i == curr_ind + 1:\n",
    "            # tokens_fixex[-1] = tokens_fixex[-1] + token.text\n",
    "            tokens_fixed.append(token.text)\n",
    "            tokens_spaces.append(token.whitespace_)\n",
    "            ents_fixed.append(f\"I-{token.ent_type_}\")\n",
    "            if idx:\n",
    "                curr_ind = idx.pop(0)\n",
    "            else:\n",
    "                curr_ind = None\n",
    "        else:\n",
    "            tokens_fixed.append(token.text)\n",
    "            tokens_spaces.append(token.whitespace_)\n",
    "            if token.ent_iob_ == \"O\":\n",
    "                ents_fixed.append(token.ent_iob_)\n",
    "            else:\n",
    "                ents_fixed.append(f\"{token.ent_iob_}-{token.ent_type_}\")\n",
    "\n",
    "    return Doc(nlp.vocab, tokens_fixed, spaces=tokens_spaces, ents=ents_fixed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_invalid_examples(predicted_examples: list[Example], verbose: bool = False) -> list[Example]:\n",
    "    merged_examples = []\n",
    "    counter = 0\n",
    "    for ind, example in enumerate(predicted_examples):\n",
    "        ents = example.predicted.ents\n",
    "\n",
    "        idx = []\n",
    "        for i, ent in enumerate(ents[:-1]):\n",
    "            next_ent = ents[i + 1]\n",
    "            if ent.end == next_ent.start and ent.label_ == next_ent.label_ and \"'\" in next_ent.text:\n",
    "                idx.append(ent.start)\n",
    "\n",
    "        if not idx:\n",
    "            merged_examples.append(example)\n",
    "            continue\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"ind: {ind}\")\n",
    "            for ent in ents:\n",
    "                print(f\"{ent.start:>3} {ent.end:>3} {ent.label_} {ent.text:12}\", end=\" \")\n",
    "                if ent.start in idx:\n",
    "                    print(\"<<<<<\")\n",
    "                else:\n",
    "                    print()\n",
    "            print(\"\\n----\\n\")\n",
    "\n",
    "        fixed_doc = get_fixed_doc(example, idx)\n",
    "\n",
    "        if verbose:\n",
    "            for ent in fixed_doc.ents:\n",
    "                print(f\"{ent.start:>3} {ent.end:>3} {ent.label_} {ent.text:12}\")\n",
    "            print(\"\\n----\\n\")\n",
    "\n",
    "        merged_examples.append(Example(fixed_doc, example.reference))\n",
    "\n",
    "    print(f\"Fixed {counter} examples.\")\n",
    "\n",
    "    return merged_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_examples = merge_invalid_examples(predicted_examples, verbose=False)\n",
    "print(f\"All examples: {len(predicted_examples)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted ind: 14 has some issues with consequent DS/DE entities\n",
    "display_doc(merged_examples[9].predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_missing_tags(\n",
    "    examples: list[Example], use_first: bool = False, use_sentence_boundaries: bool = True\n",
    ") -> list[Example]:\n",
    "    \"\"\"\n",
    "    It may happened that consequitive tags are of the same type, e.g. ... DS DE DE ...\n",
    "    which is not ideal as we cannot extract discourses from it. This function tries to add\n",
    "    missing tags to such cases based on couple of rules:\n",
    "\n",
    "    1. For the sequence of tags there musn't be any consequitive tags of the same type.\n",
    "    2. For missing tags (e.g. DS DE DE we will try to find missing DS tag so that it becomes\n",
    "       DS DE DS DE).\n",
    "    3. Missing tag is added only as a start / end of sentence. In case of situation where\n",
    "       there are couple of sentences between two consequitive tags, we can use either use first\n",
    "       approach (add missing tag to the first found sentence) or use last approach (add\n",
    "       missing tag to the last found sentence).\n",
    "    4. If use_sentence_boundaries is set to True, then we will try to add missing tags only\n",
    "       at the start / end of sentence. If this is not possible, the middle consequitive tag will be\n",
    "       removed. If use_sentence_boundaries is set to False, then the missing tag will be added to the\n",
    "       first / last untagged token before the next consequitive tag if there is no sentence boundary.\n",
    "    \"\"\"\n",
    "    fixed_examples = []\n",
    "    for ind, example in enumerate(examples):\n",
    "        print(f\"\\r{ind:3d}/{len(examples) - 1}\", end=\"\")\n",
    "\n",
    "        doc = example.predicted\n",
    "\n",
    "        last_ent = None\n",
    "        last_ent_ind = None\n",
    "\n",
    "        last_start_sent_ind = 0\n",
    "        last_end_sent_ind = 0\n",
    "\n",
    "        saved_first_token_ind = None\n",
    "\n",
    "        tokens_fixed = []\n",
    "        tokens_spaces = []\n",
    "        ents_fixed = []\n",
    "\n",
    "        for ind, token in enumerate(doc):\n",
    "            tokens_fixed.append(token.text)\n",
    "            tokens_spaces.append(token.whitespace_)\n",
    "\n",
    "            if (\n",
    "                last_start_sent_ind is not None\n",
    "                and last_start_sent_ind < ind\n",
    "                and ents_fixed\n",
    "                and last_ent is not None\n",
    "            ):\n",
    "                # Check if last idx are set on proper tokens, if not remove them\n",
    "                if last_ent == \"DE\":\n",
    "                    if ents_fixed[last_start_sent_ind] in (\"B-DS\", \"I-DS\", \"B-DE\", \"I-DE\"):\n",
    "                        last_start_sent_ind, last_end_sent_ind = None, None\n",
    "                else:\n",
    "                    if ents_fixed[last_end_sent_ind] in (\"B-DS\", \"I-DS\", \"B-DE\", \"I-DE\"):\n",
    "                        last_start_sent_ind, last_end_sent_ind = None, None\n",
    "\n",
    "            if use_first and last_start_sent_ind is None and token.text == \".\":\n",
    "                last_start_sent_ind = ind + 1\n",
    "                last_end_sent_ind = ind - 1\n",
    "\n",
    "            elif not use_first and token.text == \".\":\n",
    "                last_start_sent_ind = ind + 1\n",
    "                last_end_sent_ind = ind - 1\n",
    "\n",
    "            if not token.ent_type_:\n",
    "                if saved_first_token_ind is None:\n",
    "                    saved_first_token_ind = ind\n",
    "\n",
    "                ents_fixed.append(token.ent_iob_)\n",
    "                continue\n",
    "\n",
    "            if (token.ent_type_ == \"DS\" and last_ent == \"DE\") or (\n",
    "                token.ent_type_ == \"DE\" and last_ent == \"DS\"\n",
    "            ):\n",
    "                last_ent = token.ent_type_\n",
    "                last_ent_ind = ind\n",
    "                ents_fixed.append(f\"{token.ent_iob_}-{token.ent_type_}\")\n",
    "\n",
    "                # Okay so reset them\n",
    "                last_start_sent_ind, last_end_sent_ind = None, None\n",
    "                saved_first_token_ind = None\n",
    "                continue\n",
    "\n",
    "            if last_ent is None and token.ent_type_ == \"DS\":\n",
    "                last_ent = token.ent_type_\n",
    "                last_ent_ind = ind\n",
    "\n",
    "                last_start_sent_ind, last_end_sent_ind = None, None\n",
    "                saved_first_token_ind = None\n",
    "\n",
    "                ents_fixed.append(f\"{token.ent_iob_}-{token.ent_type_}\")\n",
    "                continue\n",
    "\n",
    "            assert not (token.ent_type_ == \"DE\" and ind == 0), \"First token must not be DE\"\n",
    "\n",
    "            # print(f\"Current token: {token.text} {token.ent_iob_}-{token.ent_type_}\")\n",
    "            # print(f\"Added ents: {ents_fixed}\")\n",
    "            if last_start_sent_ind is not None:\n",
    "                if token.ent_type_ == \"DE\":\n",
    "                    # print(\n",
    "                    #     f\"Last start sent ind: {last_start_sent_ind} - {tokens_fixed[last_start_sent_ind]} {ents_fixed[last_start_sent_ind]}\"\n",
    "                    # )\n",
    "                    assert ents_fixed[last_start_sent_ind] not in (\n",
    "                        \"B-DS\",\n",
    "                        \"I-DS\",\n",
    "                        \"B-DE\",\n",
    "                        \"I-DE\",\n",
    "                    ), \"The tag for the start of sentence is already set!\"\n",
    "                    ents_fixed[last_start_sent_ind] = \"B-DS\"\n",
    "                elif token.ent_type_ == \"DS\":\n",
    "                    # print(\n",
    "                    #     f\"Last end sent ind: {last_end_sent_ind} - {tokens_fixed[last_end_sent_ind]} {ents_fixed[last_end_sent_ind]}\"\n",
    "                    # )\n",
    "                    assert ents_fixed[last_end_sent_ind] not in (\n",
    "                        \"B-DS\",\n",
    "                        \"I-DS\",\n",
    "                        \"B-DE\",\n",
    "                        \"I-DE\",\n",
    "                    ), \"The tag for the end of sentence is already set!\"\n",
    "                    ents_fixed[last_end_sent_ind] = \"B-DE\"\n",
    "                else:\n",
    "                    assert False, \"Should not happen\"\n",
    "            else:\n",
    "                if token.ent_iob_ == \"I\" and ents_fixed[ind - 1] == f\"B-{token.ent_type_}\":\n",
    "                    # Fixed case like:\n",
    "                    # I  B-DS\n",
    "                    # 'm I-DS\n",
    "                    last_ent = token.ent_type_\n",
    "                    last_ent_ind = ind\n",
    "                    ents_fixed.append(f\"{token.ent_iob_}-{token.ent_type_}\")\n",
    "                    continue\n",
    "                elif ents_fixed[ind - 1] == f\"B-{token.ent_type_}\":\n",
    "                    # Two consequitive DS / DE tags that were not fixed so we leave\n",
    "                    # the first one and remove the second one (for DS) and\n",
    "                    # remove the first one and leave the second one (for DE)\n",
    "                    if token.ent_type_ == \"DS\":\n",
    "                        ents_fixed.append(\"O\")\n",
    "                    else:\n",
    "                        ents_fixed[ind - 1] = \"O\"\n",
    "                        ents_fixed.append(f\"{token.ent_iob_}-{token.ent_type_}\")\n",
    "                    continue\n",
    "\n",
    "                if use_sentence_boundaries:\n",
    "                    # Remove middle consequitive tag\n",
    "                    if ents_fixed[last_ent_ind].startswith(\"I-\"):\n",
    "                        # Entity is composed of more than one token\n",
    "                        curr_ind = last_ent_ind\n",
    "                        while last_ent in ents_fixed[curr_ind]:\n",
    "                            ents_fixed[curr_ind] = \"O\"\n",
    "                            curr_ind -= 1\n",
    "                    else:\n",
    "                        ents_fixed[last_ent_ind] = \"O\"\n",
    "                else:\n",
    "                    # Add missing tag to the first / last untagged token\n",
    "                    if token.ent_type_ == \"DE\":\n",
    "                        assert ents_fixed[saved_first_token_ind] not in (\n",
    "                            \"B-DS\",\n",
    "                            \"I-DS\",\n",
    "                            \"B-DE\",\n",
    "                            \"I-DE\",\n",
    "                        ), \"Start token already has a tag\"\n",
    "                        ents_fixed[saved_first_token_ind] = \"B-DS\"\n",
    "                    elif token.ent_type_ == \"DS\":\n",
    "                        assert ents_fixed[ind - 1] not in (\n",
    "                            \"B-DS\",\n",
    "                            \"I-DS\",\n",
    "                            \"B-DE\",\n",
    "                            \"I-DE\",\n",
    "                        ), \"End token already has a tag\"\n",
    "                        ents_fixed[ind - 1] = \"B-DE\"\n",
    "                    else:\n",
    "                        assert False, \"Should not happen\"\n",
    "\n",
    "            last_start_sent_ind, last_end_sent_ind = None, None\n",
    "            saved_first_token_ind = None\n",
    "\n",
    "            last_ent = token.ent_type_\n",
    "            last_ent_ind = ind\n",
    "            ents_fixed.append(f\"{token.ent_iob_}-{token.ent_type_}\")\n",
    "\n",
    "        # For cases when last token should be DE\n",
    "        if last_ent == \"DS\" and last_end_sent_ind is not None:\n",
    "            ents_fixed[last_end_sent_ind] = \"B-DE\"\n",
    "        elif last_ent == \"DS\":\n",
    "            assert ents_fixed[-1] not in (\n",
    "                \"B-DS\",\n",
    "                \"I-DS\",\n",
    "                \"B-DE\",\n",
    "                \"I-DE\",\n",
    "            ), \"Last token already has a tag\"\n",
    "            ents_fixed[-1] = \"B-DE\"\n",
    "\n",
    "        # Check if each DS tag has a DE tag and vice versa\n",
    "        ents_filtered = [ent for ent in ents_fixed if ent != \"O\" and not ent.startswith(\"I-\")]\n",
    "        assert len(ents_filtered) % 2 == 0, f\"Example {ind} has uneven number of tags: {ents_filtered}!\"\n",
    "\n",
    "        doc_fixed = Doc(nlp.vocab, tokens_fixed, spaces=tokens_spaces, ents=ents_fixed)\n",
    "        fixed_examples.append(Example(doc_fixed, example.reference))\n",
    "\n",
    "    print()\n",
    "\n",
    "    return fixed_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_doc(merged_examples[108].predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferenced_last_sents, = inference_missing_tags(\n",
    "#     [merged_examples[108]], use_first=False, use_sentence_boundaries=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenced_last_loose = inference_missing_tags(\n",
    "    merged_examples, use_first=False, use_sentence_boundaries=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenced_last_loose = inference_missing_tags(\n",
    "    merged_examples, use_first=False, use_sentence_boundaries=False\n",
    ")\n",
    "inferenced_last_sents = inference_missing_tags(\n",
    "    merged_examples, use_first=False, use_sentence_boundaries=True\n",
    ")\n",
    "inferenced_first_loose = inference_missing_tags(\n",
    "    merged_examples, use_first=True, use_sentence_boundaries=False\n",
    ")\n",
    "inferenced_first_sents = inference_missing_tags(\n",
    "    merged_examples, use_first=True, use_sentence_boundaries=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 13\n",
    "\n",
    "display_doc(merged_examples[num].predicted)\n",
    "print(\"\\nLast loose\\n\")\n",
    "display_doc(inferenced_last_loose[num].predicted)\n",
    "print(\"\\nLast sents\\n\")\n",
    "display_doc(inferenced_last_sents[num].predicted)\n",
    "print(\"\\nFirst loose\\n\")\n",
    "display_doc(inferenced_first_loose[num].predicted)\n",
    "print(\"\\nFirst sents\\n\")\n",
    "display_doc(inferenced_first_sents[num].predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (metric_path := METRICS_PATH / \"predicted_metrics.json\").exists():\n",
    "    start = perf_counter()\n",
    "    metrics = nlp.evaluate(predicted_examples, batch_size=256)\n",
    "    with open(metric_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    print(f\"Predicted done in {perf_counter() - start:.2f}s\")\n",
    "\n",
    "# ----\n",
    "\n",
    "if not (metric_path := METRICS_PATH / \"merged_metrics.json\").exists():\n",
    "    start = perf_counter()\n",
    "    metrics = nlp.evaluate(merged_examples, batch_size=256)\n",
    "    with open(metric_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    print(f\"Merged done in {perf_counter() - start:.2f}s\")\n",
    "\n",
    "# ----\n",
    "\n",
    "if not (metric_path := METRICS_PATH / \"last_loose_metrics.json\").exists():\n",
    "    start = perf_counter()\n",
    "    metrics = nlp.evaluate(inferenced_last_loose, batch_size=256)\n",
    "    with open(metric_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    print(f\"Last loose done in {perf_counter() - start:.2f}s\")\n",
    "\n",
    "# ----\n",
    "\n",
    "if not (metric_path := METRICS_PATH / \"last_sents_metrics.json\").exists():\n",
    "    start = perf_counter()\n",
    "    metrics = nlp.evaluate(inferenced_last_sents, batch_size=256)\n",
    "    with open(metric_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    print(f\"Last sents done in {perf_counter() - start:.2f}s\")\n",
    "\n",
    "# ----\n",
    "\n",
    "if not (metric_path := METRICS_PATH / \"first_loose_metrics.json\").exists():\n",
    "    start = perf_counter()\n",
    "    metrics = nlp.evaluate(inferenced_first_loose, batch_size=256)\n",
    "    with open(metric_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    print(f\"First loose done in {perf_counter() - start:.2f}s\")\n",
    "\n",
    "# ----\n",
    "\n",
    "if not (metric_path := METRICS_PATH / \"first_sents_metrics.json\").exists():\n",
    "    start = perf_counter()\n",
    "    metrics = nlp.evaluate(inferenced_first_sents, batch_size=256)\n",
    "    with open(metric_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    print(f\"First sents done in {perf_counter() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all metrics\n",
    "for path in METRICS_PATH.glob(\"*.json\"):\n",
    "    with open(path) as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    print(path.stem)\n",
    "    print(metrics, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_discourses(doc: Doc, keep_first_ds: bool = False, keep_first_de: bool = False):\n",
    "    discourses = []\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    last_ent = None\n",
    "    ents = []\n",
    "    deleted_offset = 0\n",
    "    for ind, ent in enumerate(doc.ents):\n",
    "        if ent.label_ == \"DS\" and last_ent == \"DS\":\n",
    "            if not keep_first_ds:\n",
    "                ents[ind - deleted_offset - 1] = ent\n",
    "\n",
    "            deleted_offset += 1\n",
    "            continue\n",
    "\n",
    "        if ent.label_ == \"DE\" and last_ent == \"DE\":\n",
    "            if not keep_first_de:\n",
    "                ents[ind - deleted_offset - 1] = ent\n",
    "\n",
    "            deleted_offset += 1\n",
    "            continue\n",
    "\n",
    "        ents.append(ent)\n",
    "        last_ent = ent.label_\n",
    "\n",
    "    last_tag = None\n",
    "    for ind, ent in enumerate(ents):\n",
    "        if ent.label_ == \"DS\":\n",
    "            start_pos = ent.start\n",
    "            last_tag = \"DS\"\n",
    "            continue\n",
    "\n",
    "        if ent.label_ == \"DE\":\n",
    "            assert last_tag == \"DS\", \"DE without DS\"\n",
    "            disc = \" \".join(tokens[start_pos : ent.end])\n",
    "            disc = re.sub(r\" \\.\", \".\", disc)\n",
    "            discourses.append(disc)\n",
    "            start_pos = None\n",
    "            last_tag = \"DE\"\n",
    "            continue\n",
    "\n",
    "    return discourses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = inferenced_last_loose[num].reference\n",
    "ref = extract_discourses(doc)\n",
    "ref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = inferenced_last_loose[num].predicted\n",
    "pred = extract_discourses(doc, keep_first_de=True)\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discourse_doc(doc: Doc):\n",
    "    words = [token.text for token in doc]\n",
    "\n",
    "    ents = []\n",
    "    in_disc = False\n",
    "    disc = \"DISC\"\n",
    "    for token in doc:\n",
    "        if token.ent_type_ == \"DS\":\n",
    "            in_disc = True\n",
    "            if token.ent_iob_ == \"I\":\n",
    "                ents.append(f\"I-{disc}\")\n",
    "            else:\n",
    "                ents.append(f\"B-{disc}\")\n",
    "        elif token.ent_type_ == \"DE\":\n",
    "            in_disc = False\n",
    "            ents.append(f\"I-{disc}\")\n",
    "        elif in_disc:\n",
    "            ents.append(f\"I-{disc}\")\n",
    "        else:\n",
    "            ents.append(\"O\")\n",
    "\n",
    "    return Doc(nlp.vocab, words, ents=ents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ref = inferenced_first_loose[9].reference\n",
    "disc_doc_ref = create_discourse_doc(doc_ref)\n",
    "\n",
    "doc_pred = inferenced_first_loose[9].predicted\n",
    "disc_doc_pred = create_discourse_doc(doc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ref:\\n\")\n",
    "display_doc(doc_ref)\n",
    "print(\"\\nPred:\\n\")\n",
    "display_doc(doc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_ind, example in enumerate(inferenced_first_loose):\n",
    "    doc_ref = example.reference\n",
    "    doc_pred = example.predicted\n",
    "\n",
    "    offset = 0\n",
    "    cons_idx = 0\n",
    "    for ind, token_pred in enumerate(doc_ref):\n",
    "        token_ref = doc_pred[ind + offset]\n",
    "        if token_pred.text != token_ref.text:\n",
    "            cons_idx += 1\n",
    "\n",
    "            if cons_idx > 0:\n",
    "                print(f\"Consecutive for {cons_idx} times for: {example_ind}\")\n",
    "                print(f\"Token mismatch: {token_pred.text} != {token_ref.text} (ind: {ind}, offset = {offset})\\n\")\n",
    "                break\n",
    "\n",
    "            offset += 1\n",
    "        else:\n",
    "            cons_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inferenced_first_loose[35].reference[137 - 5: 137 + 5])\n",
    "for token in inferenced_first_loose[35].predicted[137 - 5 + 7: 137 + 5 + 7]:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_doc(inferenced_first_loose[35].reference)\n",
    "print()\n",
    "display_doc(inferenced_first_loose[35].predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(doc_ref: Doc, doc_pred: Doc):\n",
    "    \"\"\"\n",
    "    1. For each original discourse, predictions which are overlapping with the original one are compared\n",
    "    2. If the overlap between a prediction and an original discourse is >= 0.5 AND overlap between an original discourse and a prediciton is >= 0.5, the prediction is a match and considered a True Positive (TP). For multiple predictions that overlap an original discourse, the prediction with the highest overlap (in both ways - pair of overlaps!) is taken only.\n",
    "    3. Any unmatched original discourses are False Negatives (FN) and any unmatched predictions are False Positives (FP).\n",
    "\n",
    "    WARNING: Tokenization is usually not the same for the reference and the prediction. This function introduces offsets to align the tokens.\n",
    "    \"\"\"\n",
    "    # Calculate offsets for non-matching tokens\n",
    "    offsets_at_pos = []\n",
    "    for ind, token_ref in enumerate(doc_ref):\n",
    "        token_pred = doc_pred[ind + len(offsets_at_pos)]\n",
    "        if token_ref.text != token_pred.text:\n",
    "            offsets_at_pos.append(ind)\n",
    "\n",
    "    \n",
    "    # {\n",
    "    #   \"0\": [(start, end), [(pred1_start, pred1_end), (pred2_start, pred2_end), ...]],   \n",
    "    #   \"1\": [(start, end), [(pred1_start, pred1_end), (pred2_start, pred2_end), ...]],\n",
    "    #   ...\n",
    "    #   \"non-overlapping\": [(start1, end1), (start2, end2), ...]\n",
    "    # }\n",
    "\n",
    "    # At this point if the offset is added to the index, the tokens should match\n",
    "    discourses_overlaps = {}\n",
    "    \n",
    "    curr_ref_start = None\n",
    "    curr_ref_end = None\n",
    "\n",
    "    overlapping_pred_discourses = {}\n",
    "\n",
    "    offset = 0\n",
    "    for ind, token_ref in enumerate(doc_ref):\n",
    "        key = len(discourses_overlaps)\n",
    "\n",
    "        if ind in offsets_at_pos:\n",
    "            offset += 1\n",
    "\n",
    "        curr_ind = ind + offset\n",
    "        token_pred = doc_pred[curr_ind]\n",
    "\n",
    "        if token_ref.ent_type_ and token_ref.ent_iob_ == \"B\":\n",
    "            # Start of a new ref discourse\n",
    "            curr_ref_start = ind\n",
    "        elif token_ref.ent_type_ and token_ref.ent_iob_ == \"I\":\n",
    "            # Continue the ref discourse\n",
    "            curr_ref_end = ind\n",
    "            discourses_overlaps[key] = [(curr_ref_start, curr_ref_end), []]\n",
    "            # Merge overlapping predictions\n",
    "            ...\n",
    "\n",
    "        if token_pred.ent_type_ and token_pred.ent_iob_ == \"B\":\n",
    "            # Start of a new pred discourse\n",
    "            pred_start = curr_ind\n",
    "\n",
    "            # Check if the prediction overlaps with any of ref discourses\n",
    "            ...\n",
    "        elif token_pred.ent_type_ and token_pred.ent_iob_ == \"I\":\n",
    "            # Continue the pred discourse\n",
    "            pred_end = curr_ind\n",
    "\n",
    "            # Check if the prediction overlaps with any of ref discourses\n",
    "            ...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_coverage(doc_ref, doc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise StopIteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "572f6aa65a5e6c51562016b4e29a85b0309fe64d207fe345880514900c6994ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
