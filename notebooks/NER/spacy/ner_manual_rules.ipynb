{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = Path.cwd().parents[2]\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(str(module_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/marek/Projects/Python/evaluating-student-writing')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from time import perf_counter\n",
    "\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marek/mambaforge/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.tokens import Doc, DocBin, Span\n",
    "from spacy.training import Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_PATH = Path.cwd() / \"metrics\"\n",
    "METRICS_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# spacy.require_gpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loader import TextLoader\n",
    "from src.model import DatasetType, Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(dataset_type=DatasetType.V1_WITH_PREDICTIONSTRING)\n",
    "nlp = spacy.load(\"models/spacy_resume/model-best/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_manual_doc(text: Text):\n",
    "    word_idx = []\n",
    "    for disc in text.discourses:\n",
    "        word_idx.extend((disc.predictionstring[0], disc.predictionstring[-1]))\n",
    "\n",
    "    ents = []\n",
    "\n",
    "    DS_token = \"B-DS\"\n",
    "    DE_token = \"B-DE\"\n",
    "    use_DS = True\n",
    "    for ind, word in enumerate(text.words):\n",
    "        if use_DS:\n",
    "            curr_token = DS_token\n",
    "        else:\n",
    "            curr_token = DE_token\n",
    "\n",
    "        if ind in word_idx:\n",
    "            ents.append(curr_token)\n",
    "            use_DS = not use_DS\n",
    "        else:\n",
    "            ents.append(\"O\")\n",
    "\n",
    "    return Doc(nlp.vocab, text.words, ents=ents)\n",
    "\n",
    "\n",
    "def display_doc(doc: Doc):\n",
    "    spacy.displacy.render(doc, style=\"ent\", jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "780"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_bin = DocBin().from_disk(\"data/NER_test.spacy\")\n",
    "len(doc_bin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples = []\n",
    "# for ind, doc in enumerate(doc_bin.get_docs(nlp.vocab)):\n",
    "#     examples.append(Example.from_dict(doc, {\"entities\": doc.ents}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/780"
     ]
    }
   ],
   "source": [
    "path_pred = Path(\"data/predicted_examples.pkl\")\n",
    "\n",
    "if not path_pred.exists():\n",
    "    predicted_examples: list[Example] = []\n",
    "    references = list(doc_bin.get_docs(nlp.vocab))\n",
    "    for ind, reference in enumerate(references):\n",
    "        print(f\"\\r{ind + 1:3d}/{len(references)}\", end=\"\")\n",
    "        doc = nlp(reference.text)\n",
    "        predicted_examples.append(Example(doc, reference))\n",
    "\n",
    "    pickle.dump(predicted_examples, open(path_pred, \"wb\"))\n",
    "\n",
    "else:\n",
    "    with open(path_pred, \"rb\") as f:\n",
    "        predicted_examples: list[Example] = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fixed_doc(example: Example, idx: list[int]):\n",
    "    doc = example.predicted\n",
    "    ents = doc.ents\n",
    "\n",
    "    tokens_fixed = []\n",
    "    tokens_spaces = []\n",
    "    ents_fixed = []\n",
    "    curr_ind = idx.pop(0)\n",
    "    for token in doc:\n",
    "        if curr_ind is not None and token.i == curr_ind + 1:\n",
    "            # tokens_fixex[-1] = tokens_fixex[-1] + token.text\n",
    "            tokens_fixed.append(token.text)\n",
    "            tokens_spaces.append(token.whitespace_)\n",
    "            ents_fixed.append(f\"I-{token.ent_type_}\")\n",
    "            if idx:\n",
    "                curr_ind = idx.pop(0)\n",
    "            else:\n",
    "                curr_ind = None\n",
    "        else:\n",
    "            tokens_fixed.append(token.text)\n",
    "            tokens_spaces.append(token.whitespace_)\n",
    "            if token.ent_iob_ == \"O\":\n",
    "                ents_fixed.append(token.ent_iob_)\n",
    "            else:\n",
    "                ents_fixed.append(f\"{token.ent_iob_}-{token.ent_type_}\")\n",
    "\n",
    "    return Doc(nlp.vocab, tokens_fixed, spaces=tokens_spaces, ents=ents_fixed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_invalid_examples(predicted_examples: list[Example], verbose: bool = False) -> list[Example]:\n",
    "    merged_examples = []\n",
    "    counter = 0\n",
    "    for ind, example in enumerate(predicted_examples):\n",
    "        ents = example.predicted.ents\n",
    "\n",
    "        idx = []\n",
    "        for i, ent in enumerate(ents[:-1]):\n",
    "            next_ent = ents[i + 1]\n",
    "            if ent.end == next_ent.start and ent.label_ == next_ent.label_ and \"'\" in next_ent.text:\n",
    "                idx.append(ent.start)\n",
    "\n",
    "        if not idx:\n",
    "            merged_examples.append(example)\n",
    "            continue\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"ind: {ind}\")\n",
    "            for ent in ents:\n",
    "                print(f\"{ent.start:>3} {ent.end:>3} {ent.label_} {ent.text:12}\", end=\" \")\n",
    "                if ent.start in idx:\n",
    "                    print(\"<<<<<\")\n",
    "                else:\n",
    "                    print()\n",
    "            print(\"\\n----\\n\")\n",
    "\n",
    "        fixed_doc = get_fixed_doc(example, idx)\n",
    "\n",
    "        if verbose:\n",
    "            for ent in fixed_doc.ents:\n",
    "                print(f\"{ent.start:>3} {ent.end:>3} {ent.label_} {ent.text:12}\")\n",
    "            print(\"\\n----\\n\")\n",
    "\n",
    "        merged_examples.append(Example(fixed_doc, example.reference))\n",
    "\n",
    "    print(f\"Fixed {counter} examples.\")\n",
    "\n",
    "    return merged_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_examples = merge_invalid_examples(predicted_examples, verbose=False)\n",
    "print(f\"All examples: {len(references)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted ind: 14 has some issues with consequent DS/DE entities\n",
    "display_doc(merged_examples[9].predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_missing_tags(\n",
    "    examples: list[Example], use_first: bool = False, use_sentence_boundaries: bool = True\n",
    ") -> list[Example]:\n",
    "    \"\"\"\n",
    "    It may happened that consequitive tags are of the same type, e.g. ... DS DE DE ...\n",
    "    which is not ideal as we cannot extract discourses from it. This function tries to add\n",
    "    missing tags to such cases based on couple of rules:\n",
    "\n",
    "    1. For the sequence of tags there musn't be any consequitive tags of the same type.\n",
    "    2. For missing tags (e.g. DS DE DE we will try to find missing DS tag so that it becomes\n",
    "       DS DE DS DE).\n",
    "    3. Missing tag is added only as a start / end of sentence. In case of situation where\n",
    "       there are couple of sentences between two consequitive tags, we can use either use first\n",
    "       approach (add missing tag to the first found sentence) or use last approach (add\n",
    "       missing tag to the last found sentence).\n",
    "    4. If use_sentence_boundaries is set to True, then we will try to add missing tags only\n",
    "       at the start / end of sentence. If this is not possible, the middle consequitive tag will be\n",
    "       removed. If use_sentence_boundaries is set to False, then the missing tag will be added to the\n",
    "       first / last untagged token before the next consequitive tag if there is no sentence boundary.\n",
    "    \"\"\"\n",
    "    fixed_examples = []\n",
    "    for ind, example in enumerate(examples):\n",
    "        print(f\"\\r{ind:3d}/{len(examples) - 1}\", end=\"\")\n",
    "\n",
    "        doc = example.predicted\n",
    "\n",
    "        last_ent = None\n",
    "        last_ent_ind = None\n",
    "\n",
    "        last_start_sent_ind = 0\n",
    "        last_end_sent_ind = 0\n",
    "\n",
    "        saved_first_token_ind = None\n",
    "\n",
    "        tokens_fixed = []\n",
    "        tokens_spaces = []\n",
    "        ents_fixed = []\n",
    "\n",
    "        for ind, token in enumerate(doc):\n",
    "            tokens_fixed.append(token.text)\n",
    "            tokens_spaces.append(token.whitespace_)\n",
    "\n",
    "            if (\n",
    "                last_start_sent_ind is not None\n",
    "                and last_start_sent_ind < ind\n",
    "                and ents_fixed\n",
    "                and last_ent is not None\n",
    "            ):\n",
    "                # Check if last idx are set on proper tokens, if not remove them\n",
    "                if last_ent == \"DE\":\n",
    "                    if ents_fixed[last_start_sent_ind] in (\"B-DS\", \"I-DS\", \"B-DE\", \"I-DE\"):\n",
    "                        last_start_sent_ind, last_end_sent_ind = None, None\n",
    "                else:\n",
    "                    if ents_fixed[last_end_sent_ind] in (\"B-DS\", \"I-DS\", \"B-DE\", \"I-DE\"):\n",
    "                        last_start_sent_ind, last_end_sent_ind = None, None\n",
    "\n",
    "            if use_first and last_start_sent_ind is None and token.text == \".\":\n",
    "                last_start_sent_ind = ind + 1\n",
    "                last_end_sent_ind = ind - 1\n",
    "\n",
    "            elif not use_first and token.text == \".\":\n",
    "                last_start_sent_ind = ind + 1\n",
    "                last_end_sent_ind = ind - 1\n",
    "\n",
    "            if not token.ent_type_:\n",
    "                if saved_first_token_ind is None:\n",
    "                    saved_first_token_ind = ind\n",
    "\n",
    "                ents_fixed.append(token.ent_iob_)\n",
    "                continue\n",
    "\n",
    "            if (token.ent_type_ == \"DS\" and last_ent == \"DE\") or (\n",
    "                token.ent_type_ == \"DE\" and last_ent == \"DS\"\n",
    "            ):\n",
    "                last_ent = token.ent_type_\n",
    "                last_ent_ind = ind\n",
    "                ents_fixed.append(f\"{token.ent_iob_}-{token.ent_type_}\")\n",
    "\n",
    "                # Okay so reset them\n",
    "                last_start_sent_ind, last_end_sent_ind = None, None\n",
    "                saved_first_token_ind = None\n",
    "                continue\n",
    "\n",
    "            if last_ent is None and token.ent_type_ == \"DS\":\n",
    "                last_ent = token.ent_type_\n",
    "                last_ent_ind = ind\n",
    "\n",
    "                last_start_sent_ind, last_end_sent_ind = None, None\n",
    "                saved_first_token_ind = None\n",
    "\n",
    "                ents_fixed.append(f\"{token.ent_iob_}-{token.ent_type_}\")\n",
    "                continue\n",
    "\n",
    "            assert not (token.ent_type_ == \"DE\" and ind == 0), \"First token must not be DE\"\n",
    "\n",
    "            # print(f\"Current token: {token.text} {token.ent_iob_}-{token.ent_type_}\")\n",
    "            if last_start_sent_ind is not None:\n",
    "                if token.ent_type_ == \"DE\":\n",
    "                    # print(\n",
    "                    #     f\"Last start sent ind: {last_start_sent_ind} - {tokens_fixed[last_start_sent_ind]} {ents_fixed[last_start_sent_ind]}\"\n",
    "                    # )\n",
    "                    assert ents_fixed[last_start_sent_ind] not in (\n",
    "                        \"B-DS\",\n",
    "                        \"I-DS\",\n",
    "                        \"B-DE\",\n",
    "                        \"I-DE\",\n",
    "                    ), \"The tag for the start of sentence is already set!\"\n",
    "                    ents_fixed[last_start_sent_ind] = \"B-DS\"\n",
    "                elif token.ent_type_ == \"DS\":\n",
    "                    # print(\n",
    "                    #     f\"Last end sent ind: {last_end_sent_ind} - {tokens_fixed[last_end_sent_ind]} {ents_fixed[last_end_sent_ind]}\"\n",
    "                    # )\n",
    "                    assert ents_fixed[last_end_sent_ind] not in (\n",
    "                        \"B-DS\",\n",
    "                        \"I-DS\",\n",
    "                        \"B-DE\",\n",
    "                        \"I-DE\",\n",
    "                    ), \"The tag for the end of sentence is already set!\"\n",
    "                    ents_fixed[last_end_sent_ind] = \"B-DE\"\n",
    "                else:\n",
    "                    assert False, \"Should not happen\"\n",
    "            else:\n",
    "                if token.ent_iob_ == \"I\" and ents_fixed[ind - 1] == f\"B-{token.ent_type_}\":\n",
    "                    # Fixed case like:\n",
    "                    # I  B-DS\n",
    "                    # 'm I-DS\n",
    "                    last_ent = token.ent_type_\n",
    "                    last_ent_ind = ind\n",
    "                    ents_fixed.append(f\"{token.ent_iob_}-{token.ent_type_}\")\n",
    "                    continue\n",
    "                elif ents_fixed[ind - 1] == f\"B-{token.ent_type_}\":\n",
    "                    # Two consequitive DS / DE tags that were not fixed so we leave\n",
    "                    # the first one and remove the second one (for DS) and\n",
    "                    # remove the first one and leave the second one (for DE)\n",
    "                    if token.ent_type_ == \"DS\":\n",
    "                        ents_fixed.append(\"O\")\n",
    "                    else:\n",
    "                        ents_fixed[ind - 1] = \"O\"\n",
    "                        ents_fixed.append(f\"{token.ent_iob_}-{token.ent_type_}\")\n",
    "                    continue\n",
    "\n",
    "                if use_sentence_boundaries:\n",
    "                    # Remove middle consequitive tag\n",
    "                    ents_fixed[last_ent_ind] = \"O\"\n",
    "                else:\n",
    "                    # Add missing tag to the first / last untagged token\n",
    "                    if token.ent_type_ == \"DE\":\n",
    "                        assert ents_fixed[saved_first_token_ind] not in (\n",
    "                            \"B-DS\",\n",
    "                            \"I-DS\",\n",
    "                            \"B-DE\",\n",
    "                            \"I-DE\",\n",
    "                        ), \"Start token already has a tag\"\n",
    "                        ents_fixed[saved_first_token_ind] = \"B-DS\"\n",
    "                    elif token.ent_type_ == \"DS\":\n",
    "                        assert ents_fixed[ind - 1] not in (\n",
    "                            \"B-DS\",\n",
    "                            \"I-DS\",\n",
    "                            \"B-DE\",\n",
    "                            \"I-DE\",\n",
    "                        ), \"End token already has a tag\"\n",
    "                        ents_fixed[ind - 1] = \"B-DE\"\n",
    "                    else:\n",
    "                        assert False, \"Should not happen\"\n",
    "\n",
    "            last_start_sent_ind, last_end_sent_ind = None, None\n",
    "            saved_first_token_ind = None\n",
    "\n",
    "            last_ent = token.ent_type_\n",
    "            last_ent_ind = ind\n",
    "            ents_fixed.append(f\"{token.ent_iob_}-{token.ent_type_}\")\n",
    "\n",
    "        doc_fixed = Doc(nlp.vocab, tokens_fixed, spaces=tokens_spaces, ents=ents_fixed)\n",
    "        fixed_examples.append(Example(doc_fixed, example.reference))\n",
    "\n",
    "    print()\n",
    "\n",
    "    return fixed_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenced_last_loose = inference_missing_tags(\n",
    "    merged_examples, use_first=False, use_sentence_boundaries=False\n",
    ")\n",
    "inferenced_last_sents = inference_missing_tags(\n",
    "    merged_examples, use_first=False, use_sentence_boundaries=True\n",
    ")\n",
    "inferenced_first_loose = inference_missing_tags(\n",
    "    merged_examples, use_first=True, use_sentence_boundaries=False\n",
    ")\n",
    "inferenced_first_sents = inference_missing_tags(\n",
    "    merged_examples, use_first=True, use_sentence_boundaries=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 15\n",
    "\n",
    "display_doc(merged_examples[num].predicted)\n",
    "print(\"\\nLast loose\\n\")\n",
    "display_doc(inferenced_last_loose[num].predicted)\n",
    "print(\"\\nLast sents\\n\")\n",
    "display_doc(inferenced_last_sents[num].predicted)\n",
    "print(\"\\nFirst loose\\n\")\n",
    "display_doc(inferenced_first_loose[num].predicted)\n",
    "print(\"\\nFirst sents\\n\")\n",
    "display_doc(inferenced_first_sents[num].predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (metric_path := METRICS_PATH / \"predicted_metrics.json\").exists():\n",
    "    start = perf_counter()\n",
    "    metrics = nlp.evaluate(predicted_examples, batch_size=256)\n",
    "    with open(metric_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    print(f\"Predicted done in {perf_counter() - start:.2f}s\")\n",
    "\n",
    "# ----\n",
    "\n",
    "if not (metric_path := METRICS_PATH / \"merged_metrics.json\").exists():\n",
    "    start = perf_counter()\n",
    "    metrics = nlp.evaluate(merged_examples, batch_size=256)\n",
    "    with open(metric_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    print(f\"Merged done in {perf_counter() - start:.2f}s\")\n",
    "\n",
    "# ----\n",
    "\n",
    "if not (metric_path := METRICS_PATH / \"last_loose_metrics.json\").exists():\n",
    "    start = perf_counter()\n",
    "    metrics = nlp.evaluate(inferenced_last_loose, batch_size=256)\n",
    "    with open(metric_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    print(f\"Last loose done in {perf_counter() - start:.2f}s\")\n",
    "\n",
    "# ----\n",
    "\n",
    "if not (metric_path := METRICS_PATH / \"last_sents_metrics.json\").exists():\n",
    "    start = perf_counter()\n",
    "    metrics = nlp.evaluate(inferenced_last_sents, batch_size=256)\n",
    "    with open(metric_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    print(f\"Last sents done in {perf_counter() - start:.2f}s\")\n",
    "\n",
    "# ----\n",
    "\n",
    "if not (metric_path := METRICS_PATH / \"first_loose_metrics.json\").exists():\n",
    "    start = perf_counter()\n",
    "    metrics = nlp.evaluate(inferenced_first_loose, batch_size=256)\n",
    "    with open(metric_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    print(f\"First loose done in {perf_counter() - start:.2f}s\")\n",
    "\n",
    "# ----\n",
    "\n",
    "if not (metric_path := METRICS_PATH / \"first_sents_metrics.json\").exists():\n",
    "    start = perf_counter()\n",
    "    metrics = nlp.evaluate(inferenced_first_sents, batch_size=256)\n",
    "    with open(metric_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    print(f\"First sents done in {perf_counter() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all metrics\n",
    "for path in METRICS_PATH.glob(\"*.json\"):\n",
    "    with open(path) as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    print(path.stem)\n",
    "    print(metrics, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_discourses(doc: Doc, keep_first_ds: bool = False, keep_first_de: bool = False):\n",
    "    discourses = []\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    last_ent = None\n",
    "    ents = []\n",
    "    deleted_offset = 0\n",
    "    for ind, ent in enumerate(doc.ents):\n",
    "        if ent.label_ == \"DS\" and last_ent == \"DS\":\n",
    "            if not keep_first_ds:\n",
    "                ents[ind - deleted_offset - 1] = ent\n",
    "\n",
    "            deleted_offset += 1\n",
    "            continue\n",
    "\n",
    "        if ent.label_ == \"DE\" and last_ent == \"DE\":\n",
    "            if not keep_first_de:\n",
    "                ents[ind - deleted_offset - 1] = ent\n",
    "\n",
    "            deleted_offset += 1\n",
    "            continue\n",
    "\n",
    "        ents.append(ent)\n",
    "        last_ent = ent.label_\n",
    "\n",
    "    last_tag = None\n",
    "    for ind, ent in enumerate(ents):\n",
    "        if ent.label_ == \"DS\":\n",
    "            start_pos = ent.start\n",
    "            last_tag = \"DS\"\n",
    "            continue\n",
    "\n",
    "        if ent.label_ == \"DE\":\n",
    "            assert last_tag == \"DS\", \"DE without DS\"\n",
    "            disc = \" \".join(tokens[start_pos : ent.end])\n",
    "            disc = re.sub(r\" \\.\", \".\", disc)\n",
    "            discourses.append(disc)\n",
    "            start_pos = None\n",
    "            last_tag = \"DE\"\n",
    "            continue\n",
    "\n",
    "    return discourses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = inferenced_last_loose[num].reference\n",
    "ref = extract_discourses(doc)\n",
    "ref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = inferenced_last_loose[num].predicted\n",
    "pred = extract_discourses(doc, keep_first_de=True)\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discourse_doc(doc: Doc):\n",
    "    words = [token.text for token in doc]\n",
    "\n",
    "    ents = []\n",
    "    in_disc = False\n",
    "    disc = \"DISC\"\n",
    "    for token in doc:\n",
    "        if token.ent_type_ == \"DS\":\n",
    "            in_disc = True\n",
    "            if token.ent_iob_ == \"I\":\n",
    "                ents.append(f\"I-{disc}\")\n",
    "            else:\n",
    "                ents.append(f\"B-{disc}\")\n",
    "        elif token.ent_type_ == \"DE\":\n",
    "            in_disc = False\n",
    "            ents.append(f\"I-{disc}\")\n",
    "        elif in_disc:\n",
    "            ents.append(f\"I-{disc}\")\n",
    "        else:\n",
    "            ents.append(\"O\")\n",
    "\n",
    "    return Doc(nlp.vocab, words, ents=ents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_doc = inferenced_first_loose[9].reference\n",
    "disc_doc = create_discourse_doc(disc_doc)\n",
    "display_doc(disc_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_doc = inferenced_first_loose[9].predicted\n",
    "disc_doc = create_discourse_doc(disc_doc)\n",
    "display_doc(disc_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise StopIteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_phrase = (\n",
    "    \"In the United States approximately 9 people are killed and over a thousand were injured\"\n",
    ")\n",
    "for text in loader.iterate(purify_discourses=True, purify_text=True, verbose=True):\n",
    "    if search_phrase in text.text:\n",
    "        print(text.id)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# 2F159741CBBE text is good for analyzing in the document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = loader.load_text_with_id(\"2F159741CBBE\")\n",
    "for disc in text.discourses:\n",
    "    print(disc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "572f6aa65a5e6c51562016b4e29a85b0309fe64d207fe345880514900c6994ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
